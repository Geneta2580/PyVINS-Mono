from hmac import new
import queue
import numpy as np
import gtsam
from gtsam.symbol_shorthand import X, V, B, L
# from gtsam_unstable import IncrementalFixedLagSmoother, FixedLagSmootherKeyTimestampMap
from gtsam import IncrementalFixedLagSmoother

import re
from utils.debug import Debugger
import time

class Backend:
    def __init__(self, global_central_map, config, imu_processor):
        self.global_central_map = global_central_map
        self.config = config

        # ä½¿ç”¨ iSAM2 ä½œä¸ºä¼˜åŒ–å™¨
        self.lag_window_size = config.get('lag_window_size', 9) # ä¼˜åŒ–å™¨çš„æ»‘çª—
        parameters = gtsam.ISAM2Params()
        parameters.setRelinearizeThreshold(0.01) 
        parameters.relinearizeSkip = 1
        self.smoother = IncrementalFixedLagSmoother(self.lag_window_size, parameters) # è‡ªåŠ¨è¾¹ç¼˜åŒ–
        
        # é²æ£’å› å­
        self.visual_noise_sigma = config.get('visual_noise_sigma', 2.0)
        self.visual_noise = gtsam.noiseModel.Isotropic.Sigma(2, self.visual_noise_sigma)
        self.visual_robust_noise = gtsam.noiseModel.Robust.Create(gtsam.noiseModel.mEstimator.Huber.Create(1.345), self.visual_noise)

        # æ˜¯å¦ä½¿ç”¨æ·±åº¦é™æƒ
        self.use_depth_weight = config.get('use_depth_weight', False)
        # æ·»åŠ æ·±åº¦é™æƒå‚æ•°
        self.depth_weight_base = config.get('depth_weight_base', 5.0)  # åŸºç¡€æ·±åº¦é˜ˆå€¼ï¼ˆç±³ï¼‰
        self.depth_weight_max = config.get('depth_weight_max', 3.0)  # æœ€å¤§å™ªå£°å€æ•°
        self.depth_weight_power = config.get('depth_weight_power', 1.5)  # æ·±åº¦æƒé‡æŒ‡æ•°
        self.new_landmark_inflation_ratio = config.get('new_landmark_inflation_ratio', 5.0)

        # é¢„ä¼˜åŒ–æœ€å¤§é‡æŠ•å½±è¯¯å·®
        self.rejection_threshold = config.get('rejection_threshold', 400.0)

        # çŠ¶æ€ä¸idç®¡ç†
        self.kf_id_to_gtsam_id = {}
        self.landmark_id_to_gtsam_id = {}
        self.next_gtsam_kf_id = 0
        self.factor_indices_to_remove = []

        # è·å–ç›¸æœºå†…ã€å¤–å‚
        cam_intrinsics = np.asarray(self.config.get('cam_intrinsics')).reshape(3, 3)
        self.K = gtsam.Cal3_S2(cam_intrinsics[0, 0], cam_intrinsics[1, 1], 0, 
                               cam_intrinsics[0, 2], cam_intrinsics[1, 2])

        T_bc_raw = self.config.get('T_bc', np.eye(4).flatten().tolist())
        self.T_bc = np.asarray(T_bc_raw).reshape(4, 4)
        self.body_T_cam = gtsam.Pose3(self.T_bc)
        self.cam_T_body = self.body_T_cam.inverse()

        # å­˜å‚¨æœ€æ–°çš„ä¼˜åŒ–åçš„åç½®ï¼Œç”¨äºIMUé¢„ç§¯åˆ†
        self.latest_bias = gtsam.imuBias.ConstantBias()

        # å®šä¹‰è¦è®°å½•çš„åˆ—
        log_columns = [
            "gtsam_id", "pos_x", "pos_y", "pos_z",
            "vel_x", "vel_y", "vel_z",
            "bias_acc_x", "bias_acc_y", "bias_acc_z",
            "bias_gyro_x", "bias_gyro_y", "bias_gyro_z",
            "new_factors_error"
        ]
        # åˆå§‹åŒ–Debugger
        self.logger = Debugger(self.config, file_prefix="backend_state", column_names=log_columns)

    # å…³é”®å¸§idæ˜ å°„åˆ°å›¾çš„id
    def _get_kf_gtsam_id(self, kf_id):
        if kf_id not in self.kf_id_to_gtsam_id:
            self.kf_id_to_gtsam_id[kf_id] = self.next_gtsam_kf_id
            self.next_gtsam_kf_id += 1
        return self.kf_id_to_gtsam_id[kf_id]

    # è·¯æ ‡ç‚¹idæ˜ å°„åˆ°å›¾çš„id
    def _get_lm_gtsam_id(self, lm_id):
        if lm_id not in self.landmark_id_to_gtsam_id:
            self.landmark_id_to_gtsam_id[lm_id] = lm_id
        return self.landmark_id_to_gtsam_id[lm_id]

    def get_latest_optimized_state(self):
        if self.next_gtsam_kf_id == 0:
            return None, None, None
        
        latest_gtsam_id = self.next_gtsam_kf_id - 1

        result = self.smoother.calculateEstimate()

        try:
            pose = result.atPose3(X(latest_gtsam_id))
            velocity = result.atVector(V(latest_gtsam_id))
            bias = result.atConstantBias(B(latest_gtsam_id))
            # print(f"ã€Backendã€‘: Latest optimized state: pose: {pose.matrix()}, velocity: {velocity}, bias: {bias}")
            return pose, velocity, bias
        except Exception as e:
            print(f"[Error][Backend] Failed to retrieve latest state for gtsam_id {latest_gtsam_id}: {e}")
            return None, None, None

    def update_estimator_map(self, keyframe_window, landmarks):
        print("ã€Backendã€‘: Syncing optimized results back to Estimator...")
        optimized_results = self.smoother.calculateEstimate()

        # æ›´æ–°å…³é”®å¸§ä½å§¿
        for kf in keyframe_window:
           # è·å–å¾…æ›´æ–°å…³é”®å¸§çš„gtsam_id
            gtsam_id = self.kf_id_to_gtsam_id.get(kf.get_id())
            if gtsam_id is not None and optimized_results.exists(X(gtsam_id)):
                
                # ä»ä¼˜åŒ–ç»“æœä¸­è·å–æœ€æ–°çš„IMUä½å§¿ T_w_bå¹¶æ›´æ–°
                pose_w_b = optimized_results.atPose3(X(gtsam_id))
                kf.set_global_pose(pose_w_b.matrix())

        # æ›´æ–°è·¯æ ‡ç‚¹åæ ‡
        for lm_id, landmark_obj in landmarks.items():
            gtsam_id = self._get_lm_gtsam_id(lm_id)
            if gtsam_id is not None and optimized_results.exists(L(gtsam_id)):
                # 1. ä»ä¼˜åŒ–ç»“æœä¸­è·å–æœ€æ–°çš„3Dåæ ‡
                optimized_position = optimized_results.atPoint3(L(gtsam_id))
                # 2. è°ƒç”¨å¯¹è±¡çš„æ–¹æ³•æ¥æ›´æ–°å…¶å†…éƒ¨çŠ¶æ€
                landmark_obj.set_triangulated(optimized_position)
                # print(f"ã€Backendã€‘: Updated landmark {lm_id} to {optimized_position}")

    def remove_stale_landmarks(self, unhealty_lm_ids, unhealty_lm_ids_depth, 
                                unhealty_lm_ids_reproj, oldest_kf_id_in_window):
        print(f"ã€Backendã€‘: æ¥æ”¶åˆ°ç§»é™¤ {len(unhealty_lm_ids)} ä¸ªé™ˆæ—§è·¯æ ‡ç‚¹çš„æŒ‡ä»¤ã€‚")
        if not unhealty_lm_ids:
            return

        # ä¸å†æ‰‹åŠ¨åˆ é™¤å› å­ï¼
        # åŸå› ï¼šæ‰‹åŠ¨åˆ é™¤å› å­ä¼šä¸Fixed-Lag Smootherçš„è‡ªåŠ¨è¾¹ç¼˜åŒ–æœºåˆ¶å†²çª
        # å¯¼è‡´ IndexError: map::at
        
        # åªåˆ é™¤IDæ˜ å°„ï¼Œé˜»æ­¢è¿™äº›landmarkå†æ¬¡è¢«æ·»åŠ åˆ°å›¾ä¸­
        for lm_id in unhealty_lm_ids:
            if lm_id in self.landmark_id_to_gtsam_id:
                del self.landmark_id_to_gtsam_id[lm_id]
                print(f"ã€Backendã€‘: å·²ç§»é™¤ landmark {lm_id} çš„IDæ˜ å°„")

        print(f"ã€Backendã€‘: æˆåŠŸæ ‡è®° {len(unhealty_lm_ids)} ä¸ªè·¯æ ‡ç‚¹ä¸ºå¾…æ¸…ç†çŠ¶æ€")
        print(f"ã€Backendã€‘: Fixed-Lag Smoother å°†åœ¨æ»‘çª—ç§»åŠ¨æ—¶è‡ªåŠ¨æ¸…ç†è¿™äº›landmark")

        # # åˆ é™¤å› å­é€»è¾‘
        # print(f"ã€Backendã€‘: æ¥æ”¶åˆ°ç§»é™¤ {len(unhealty_lm_ids)} ä¸ªé™ˆæ—§è·¯æ ‡ç‚¹çš„æŒ‡ä»¤ã€‚")
        # if not unhealty_lm_ids:
        #     return

        # graph = self.smoother.getFactors()
        # factor_indices_to_remove = []
        # unhealty_lm_keys = {L(self._get_lm_gtsam_id(lm_id)) for lm_id in unhealty_lm_ids}
        # unhealty_lm_keys_depth = {L(self._get_lm_gtsam_id(lm_id)) for lm_id in unhealty_lm_ids_depth}
        # unhealty_lm_keys_reproj = {L(self._get_lm_gtsam_id(lm_id)) for lm_id in unhealty_lm_ids_reproj}

        # oldest_gtsam_key = None
        # if oldest_kf_id_in_window is not None and oldest_kf_id_in_window in self.kf_id_to_gtsam_id:
        #     oldest_gtsam_key = X(self._get_kf_gtsam_id(oldest_kf_id_in_window))
        #     print(f"ã€Backendã€‘: æœ€æ—§çš„å…³é”®å¸§çš„gtsam_id: {oldest_gtsam_key}")

        # # æ”¶é›†éœ€è¦åˆ é™¤çš„å› å­
        # for i in range(graph.size()):
        #     factor = graph.at(i)
        #     if factor is not None:
        #         factor_type = factor.__class__.__name__
                
        #         # åªåˆ é™¤æŠ•å½±å› å­ï¼Œç»ä¸åˆ é™¤è¾¹ç¼˜åŒ–å› å­ã€IMUå› å­ç­‰
        #         if factor_type != 'GenericProjectionFactorCal3_S2':
        #             continue
                
        #         for key in factor.keys():
        #             if key in unhealty_lm_keys_depth or key in unhealty_lm_keys_reproj:
        #                 key_str = ", ".join([gtsam.DefaultKeyFormatter(k) for k in factor.keys()])
        #                 print(f"  [æ ‡è®°åˆ é™¤] Index: {i}, ç±»å‹: {factor_type}, è¿æ¥: [{key_str}]")
        #                 factor_indices_to_remove.append(i)
        #                 break

        # # å…³é”®ä¿®æ”¹ï¼šåªåˆ é™¤å› å­ï¼Œä¸è¦å°è¯•æ“ä½œå˜é‡çš„æ—¶é—´æˆ³
        # if factor_indices_to_remove:
        #     empty_graph = gtsam.NonlinearFactorGraph()
        #     empty_values = gtsam.Values()
        #     # empty_stamps = FixedLagSmootherKeyTimestampMap()
        #     empty_stamps = {}
            
        #     self.smoother.update(empty_graph, empty_values, empty_stamps, factor_indices_to_remove)
        #     print(f"ã€Backendã€‘: æˆåŠŸç§»é™¤ {len(factor_indices_to_remove)} ä¸ªæ·±åº¦ä¸ºè´Ÿçš„è·¯æ ‡ç‚¹çš„å› å­")

        # # åˆ é™¤IDæ˜ å°„ - ä¿®æ­£ï¼šåªåˆ é™¤é‚£äº›å®é™…åˆ é™¤äº†å› å­çš„landmark
        # for lm_id in unhealty_lm_ids:  # æ”¹ä¸º unhealty_lm_ids_depth
        #     if lm_id in self.landmark_id_to_gtsam_id:
        #         del self.landmark_id_to_gtsam_id[lm_id]

        # print(f"ã€Backendã€‘: æˆåŠŸç§»é™¤ {len(unhealty_lm_ids)} ä¸ªè·¯æ ‡ç‚¹çš„å› å­")
        

    def initialize_optimize(self, initial_keyframes, initial_imu_factors, initial_landmarks, initial_velocities, initial_bias):
        print("ã€Backendã€‘: Initializing optimize...")

        graph = gtsam.NonlinearFactorGraph()
        estimates = gtsam.Values()
        
        # initial_window_stamps = FixedLagSmootherKeyTimestampMap()
        initial_window_stamps = {}

        for i, kf in enumerate(initial_keyframes):
            kf_gtsam_id = self._get_kf_gtsam_id(kf.get_id())

            # ä»åˆå§‹åŒ–ç»“æœä¸­è·å–ä½å§¿ã€é€Ÿåº¦å’Œåç½®
            T_wb = gtsam.Pose3(kf.get_global_pose())
            # initial_velocities æ˜¯ä¸€ä¸ªæ‰å¹³åŒ–çš„æ•°ç»„ï¼Œæ¯3ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªé€Ÿåº¦å‘é‡
            velocity = initial_velocities[i*3 : i*3+3]
            
            # æ‰€æœ‰å¸§ä½¿ç”¨ç›¸åŒçš„åˆå§‹åç½®
            bias = initial_bias

            # æ·»åŠ åˆå§‹ä¼°è®¡å€¼
            estimates.insert(X(kf_gtsam_id), T_wb)
            estimates.insert(V(kf_gtsam_id), velocity)
            estimates.insert(B(kf_gtsam_id), bias)

            # æ·»åŠ æ»‘çª—è®°å½•
            initial_window_stamps[X(kf_gtsam_id)] = float(kf_gtsam_id)
            initial_window_stamps[V(kf_gtsam_id)] = float(kf_gtsam_id)
            initial_window_stamps[B(kf_gtsam_id)] = float(kf_gtsam_id)

            # ä¸ºç¬¬ä¸€å¸§æ·»åŠ å¼ºå…ˆéªŒ
            if kf_gtsam_id == 0:
                prior_pose_noise = gtsam.noiseModel.Diagonal.Sigmas(np.array([1e-4]*3 + [1e-2]*3))
                prior_vel_noise = gtsam.noiseModel.Diagonal.Sigmas(np.array([2e-2] * 3))
                prior_bias_noise = gtsam.noiseModel.Diagonal.Sigmas(np.array([1e-1]*3 + [1e-2]*3))
                graph.add(gtsam.PriorFactorPose3(X(0), T_wb, prior_pose_noise))
                graph.add(gtsam.PriorFactorVector(V(0), velocity, prior_vel_noise))
                graph.add(gtsam.PriorFactorConstantBias(B(0), bias, prior_bias_noise))
        
        # ä¸ºæ¯ä¸€ä¸ªlandmarkè®¾ç½®æ»‘çª—è®°å½•
        last_gtsam_id = self._get_kf_gtsam_id(initial_keyframes[-1].get_id())
        for lm_id in initial_landmarks.keys():
            lm_gtsam_id = self._get_lm_gtsam_id(lm_id)
            initial_window_stamps[L(lm_gtsam_id)] = float(last_gtsam_id) # è®¾ä¸ºæœ€åä¸€å¸§çš„ID

        # æ·»åŠ æ‰€æœ‰åˆå§‹IMUå› å­
        for factor_data in initial_imu_factors:
            start_kf = next(kf for kf in initial_keyframes if kf.get_timestamp() == factor_data['start_kf_timestamp'])
            end_kf = next(kf for kf in initial_keyframes if kf.get_timestamp() == factor_data['end_kf_timestamp'])
            gtsam_id1 = self._get_kf_gtsam_id(start_kf.get_id())
            gtsam_id2 = self._get_kf_gtsam_id(end_kf.get_id())
            pim = factor_data['imu_preintegration']
            graph.add(gtsam.CombinedImuFactor(X(gtsam_id1), V(gtsam_id1), X(gtsam_id2), V(gtsam_id2), B(gtsam_id1), B(gtsam_id2), pim))

        # æ·»åŠ æ‰€æœ‰åˆå§‹è·¯æ ‡ç‚¹å˜é‡å’Œè§†è§‰å› å­
        for lm_id, lm_3d_pos in initial_landmarks.items():
            lm_gtsam_id = self._get_lm_gtsam_id(lm_id)
            estimates.insert(L(lm_gtsam_id), lm_3d_pos)

        for kf in initial_keyframes:
            kf_gtsam_id = self._get_kf_gtsam_id(kf.get_id())
            for lm_id, pt_2d in zip(kf.get_visual_feature_ids(), kf.get_visual_features()):
                # åªå¤„ç†æœ¬æ¬¡ä¼˜åŒ–ä¸­æ–°æ·»åŠ çš„landmark
                if lm_id in initial_landmarks:
                    lm_gtsam_id = self._get_lm_gtsam_id(lm_id)
                    # è®¡ç®—æ·±åº¦å¹¶åº”ç”¨é™æƒ
                    T_wb = gtsam.Pose3(kf.get_global_pose()) # è·å–å…³é”®å¸§ä½å§¿ç”¨äºæ·±åº¦è®¡ç®—
                    current_lm_pos = initial_landmarks[lm_id]
                    depth = self._compute_landmark_depth(current_lm_pos, T_wb)
                    # è¿™é‡Œå°†åˆå§‹åŒ–çš„ç‚¹æ ‡è®°ä¸ºFalse
                    weighted_noise = self._get_adaptive_noise(depth, False)

                    factor = gtsam.GenericProjectionFactorCal3_S2(
                        pt_2d, weighted_noise, X(kf_gtsam_id), L(lm_gtsam_id), 
                        self.K, body_P_sensor=self.body_T_cam
                    )
                    graph.add(factor)

        # æ‰§è¡ŒiSAM2çš„ç¬¬ä¸€æ¬¡æ›´æ–°ï¼ˆæ‰¹é‡æ¨¡å¼ï¼‰
        print(f"ã€Backendã€‘: Initializing iSAM2 with {graph.size()} new factors and {estimates.size()} new values...")
        
        try:
            start_time = time.time()
            self.smoother.update(graph, estimates, initial_window_stamps)
            end_time = time.time()
            print(f"ã€Backend Timerã€‘: Initial optimization took { (end_time - start_time) * 1000:.3f} ms.")
        except RuntimeError as e:
            print("\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
            print("!!!!!!!!!! INITIALIZATION FAILED !!!!!!!!!!!!!!")
            print(f"ERROR: {e}")
            return # å¤±è´¥æ—¶å¿…é¡»è¿”å›

        # æ›´æ–°æœ€æ–°bias
        latest_pose, latest_vel, latest_bias = self.get_latest_optimized_state()
        print(f"ã€Backendã€‘: Latest optimized state: pose: {latest_pose.matrix()}, velocity: {latest_vel}, bias: {latest_bias}")

        latest_gtsam_id = self.next_gtsam_kf_id - 1
        print(f"ã€Backendã€‘: Latest gtsam_id: {latest_gtsam_id}")
        if latest_bias is not None:
            self.latest_bias = latest_bias
        print("ã€Backendã€‘: Initial graph optimization complete.")

        # è®°å½•ä¼˜åŒ–çŠ¶æ€
        new_factors_error = self._log_optimization_error(graph)
        self._log_state_and_errors(latest_gtsam_id, latest_pose, latest_vel, latest_bias, new_factors_error)


    def optimize_incremental(self, last_keyframe, new_keyframe, new_imu_factors, 
                            new_landmarks, new_visual_factors, initial_state_guess, is_stationary, oldest_kf_id_in_window):
        new_graph = gtsam.NonlinearFactorGraph()
        new_estimates = gtsam.Values()
        current_isam_values = self.smoother.calculateEstimate()
        new_window_stamps = {}

        # æ·»åŠ æ–°å…³é”®å¸§çš„çŠ¶æ€å˜é‡ï¼Œä½¿ç”¨IMUé¢„æµ‹å€¼ä½œä¸ºåˆå§‹ä¼°è®¡
        kf_gtsam_id = self._get_kf_gtsam_id(new_keyframe.get_id())
        T_wb_guess, vel_guess, bias_guess = initial_state_guess

        # æ£€æŸ¥å…³é”®å¸§æ˜¯å¦å·²ç»åœ¨å›¾ä¸­å­˜åœ¨ï¼Œé¿å…é‡å¤æ·»åŠ ï¼ˆé˜²å¾¡æ€§æ£€æŸ¥ï¼‰
        if not current_isam_values.exists(X(kf_gtsam_id)) or not current_isam_values.exists(V(kf_gtsam_id)) or not current_isam_values.exists(B(kf_gtsam_id)):
            new_estimates.insert(X(kf_gtsam_id), T_wb_guess)
            new_estimates.insert(V(kf_gtsam_id), vel_guess)
            new_estimates.insert(B(kf_gtsam_id), bias_guess)

            # æ·»åŠ æ»‘çª—è®°å½•
            new_window_stamps[X(kf_gtsam_id)] = float(kf_gtsam_id)
            new_window_stamps[V(kf_gtsam_id)] = float(kf_gtsam_id)
            new_window_stamps[B(kf_gtsam_id)] = float(kf_gtsam_id)
        else:
            print(f"ã€Backendã€‘: Warning: Keyframe {new_keyframe.get_id()} (gtsam_id={kf_gtsam_id}) already exists in graph. Skipping variable insertion.")
            # å¦‚æœå…³é”®å¸§å·²å­˜åœ¨ï¼Œä»ç„¶éœ€è¦æ›´æ–°æ»‘çª—æ—¶é—´æˆ³ï¼ˆå¦‚æœFixed-Lag Smootheréœ€è¦ï¼‰
            # æ³¨æ„ï¼šè¿™é‡Œä¸æ·»åŠ å˜é‡ï¼Œåªæ›´æ–°æ—¶é—´æˆ³ï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰

        # if not is_stationary:
        # æ·»åŠ IMUå› å­
        last_kf_gtsam_id = self._get_kf_gtsam_id(last_keyframe.get_id())
        pim = new_imu_factors['imu_preintegration']
        imu_factor = gtsam.CombinedImuFactor(
            X(last_kf_gtsam_id), V(last_kf_gtsam_id), X(kf_gtsam_id), V(kf_gtsam_id),
            B(last_kf_gtsam_id), B(kf_gtsam_id), pim)
        new_graph.add(imu_factor)

        # æ·»åŠ æ–°è·¯æ ‡ç‚¹é¡¶ç‚¹ï¼Œæ³¨æ„è¿™é‡Œæ·»åŠ çš„é¡¶ç‚¹åªåœ¨new_estimatesä¸­è¿˜æ²¡æœ‰è¿›å…¥isam2çš„å›¾
        # if not is_stationary:
        added_new_landmark_gtsam_ids = set()

        for lm_id, lm_3d_pos in new_landmarks.items():            
            # å¢åŠ ä¸€ä¸ªNaN/Infçš„æ˜¾å¼æ£€æŸ¥ï¼Œè¿™å¯¹äºè°ƒè¯•å´©æºƒè‡³å…³é‡è¦
            if np.isnan(lm_3d_pos).any() or np.isinf(lm_3d_pos).any():
                print(f"ğŸ”¥ ã€Backendã€‘[è‡´å‘½è­¦å‘Š]: è·¯æ ‡ç‚¹ L{lm_id} çš„åˆå§‹å€¼æ— æ•ˆ (NaN/Inf)ï¼ä¼˜åŒ–å³å°†å› æ­¤å´©æºƒï¼")
                continue  # ç›´æ¥è·³è¿‡æ— æ•ˆçš„landmark

            lm_gtsam_id = self._get_lm_gtsam_id(lm_id)
            # ---!!!--- åœ¨æ­¤å¤„æ·»åŠ æ‚¨è¦çš„æ—¥å¿— ---!!!---
            # æ‰“å°å³å°†é€å…¥ä¼˜åŒ–å™¨çš„è·¯æ ‡ç‚¹çš„å€¼
            # print(f"ğŸ•µï¸â€ ã€Backendã€‘: ä¼˜åŒ–å™¨å³å°†å¤„ç†æ–°è·¯æ ‡ç‚¹ L{lm_id}ï¼Œå…¶ä¸‰è§’åŒ–åˆå§‹å€¼ä¸º: {lm_3d_pos}")

            # æ£€æŸ¥ï¼š1) ä¸åœ¨æ—§å›¾ä¸­ï¼Œ2) è¿˜æ²¡è¢«æ·»åŠ è¿‡ ç¡®ä¿é¡¶ç‚¹åªè¢«æ·»åŠ ä¸€æ¬¡
            if not current_isam_values.exists(L(lm_gtsam_id)):
                new_estimates.insert(L(lm_gtsam_id), lm_3d_pos)
                # æ·»åŠ æ–°è·¯æ ‡ç‚¹çš„æ»‘çª—è®°å½•
                new_window_stamps[L(lm_gtsam_id)] = float(kf_gtsam_id)
                added_new_landmark_gtsam_ids.add(lm_gtsam_id)
        
        # å¦‚æœä¸€ä¸ªæ–°è·¯æ ‡ç‚¹åœ¨ estimates é‡Œï¼Œä½†æ‰€æœ‰å› å­éƒ½è¢« chi2 æ‹’ç»ï¼Œå¿…é¡»å°†å…¶ä» estimates ç§»é™¤
        # å¦åˆ™ä¼šå¯¼è‡´ iSAM2 é‡åˆ°æ— çº¦æŸå˜é‡è€Œå¥‡å¼‚/å´©æºƒ
        valid_new_landmarks = set()

        # -------------------------------------------------------------------------
        # ã€æ–°å¢é€»è¾‘ã€‘: å› å­é˜²ç«å¢™ (Factor Firewall)
        # -------------------------------------------------------------------------
        
        valid_visual_factors = []
        bad_landmarks = set() # è®°å½•åç‚¹

        for kf_id, lm_id, pt_2d in new_visual_factors:
            # 1. åŸºç¡€æ£€æŸ¥
            if lm_id not in self.landmark_id_to_gtsam_id:
                continue
            
            kf_gtsam_id = self._get_kf_gtsam_id(kf_id)
            lm_gtsam_id = self._get_lm_gtsam_id(lm_id)

            # 2. å‡†å¤‡è®¡ç®—è¯¯å·®æ‰€éœ€çš„ä¸´æ—¶å˜é‡
            # æˆ‘ä»¬éœ€è¦è·å– kf çš„ä½å§¿ å’Œ lm çš„ä½ç½®
            # æƒ…å†µA: å˜é‡åœ¨ new_estimates ä¸­ (æœ¬å¸§æ–°åŠ çš„)
            # æƒ…å†µB: å˜é‡åœ¨ current_isam_values ä¸­ (è€å˜é‡)
            
            pose = None
            if new_estimates.exists(X(kf_gtsam_id)):
                pose = new_estimates.atPose3(X(kf_gtsam_id))
            elif current_isam_values.exists(X(kf_gtsam_id)):
                pose = current_isam_values.atPose3(X(kf_gtsam_id))
            
            point = None
            is_new_point = False
            if new_estimates.exists(L(lm_gtsam_id)):
                point = new_estimates.atPoint3(L(lm_gtsam_id))
                is_new_point = True
            elif current_isam_values.exists(L(lm_gtsam_id)):
                point = current_isam_values.atPoint3(L(lm_gtsam_id))
            
            # å¦‚æœæˆ‘ä»¬æ‰¾ä¸åˆ°ä½å§¿æˆ–ç‚¹ï¼Œå°±æ²¡æ³•è®¡ç®—è¯¯å·®ï¼Œåªèƒ½å…ˆè·³è¿‡ (æˆ–ä¿å®ˆæ·»åŠ )
            if pose is None or point is None:
                continue

            # 3. æ„é€ ä¸´æ—¶å› å­è®¡ç®—è¯¯å·®
            # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬è¿˜æ²¡çœŸçš„åŠ åˆ° new_graphï¼Œåªæ˜¯æ¨¡æ‹Ÿä¸€ä¸‹
            if self.use_depth_weight:
                depth = self._compute_landmark_depth(point, pose)
            else:
                depth = None
            
            # ä½¿ç”¨ä¸¥æ ¼çš„å™ªå£°æ¨¡å‹è¿›è¡Œæ£€æµ‹ (ä¸åŠ  Hubberï¼Œçœ‹åŸå§‹è¯¯å·®)
            check_noise = gtsam.noiseModel.Isotropic.Sigma(2, 1.0) 
            temp_factor = gtsam.GenericProjectionFactorCal3_S2(
                pt_2d, check_noise, X(kf_gtsam_id), L(lm_gtsam_id), 
                self.K, body_P_sensor=self.body_T_cam
            )
            
            # æ„é€ ä¸´æ—¶ Values
            temp_values = gtsam.Values()
            temp_values.insert(X(kf_gtsam_id), pose)
            temp_values.insert(L(lm_gtsam_id), point)
            
            try:
                # è®¡ç®—æœªç»é²æ£’æ ¸æŠ‘åˆ¶çš„åŸå§‹åƒç´ è¯¯å·®
                error = temp_factor.error(temp_values)
            except:
                error = float('inf')

            # 4. åˆ¤å†³æ—¶åˆ»ï¼
            # é˜ˆå€¼è®¾å®šï¼š
            # Error = 0.5 * (u-u')^2 / sigma^2
            # å¦‚æœ sigma=1, error=50 æ„å‘³ç€åƒç´ è¯¯å·® sqrt(100) = 10 åƒç´ 
            # error=1618 æ„å‘³ç€åƒç´ è¯¯å·®ææå¤§
            
            REJECTION_THRESHOLD = self.rejection_threshold  # å¯¹åº”çº¦ 10 åƒç´ çš„é‡æŠ•å½±è¯¯å·®
            
            if error > REJECTION_THRESHOLD:
                # è¿™æ˜¯ä¸€ä¸ªåå› å­ï¼
                print(f"ğŸ”¥ [Firewall] æ‹¦æˆªåå› å­! KF{kf_id}-LM{lm_id}, Error: {error:.2f}")
                bad_landmarks.add(lm_id) # æ ‡è®°è¿™ä¸ªç‚¹æœ‰é—®é¢˜
                
                # å¦‚æœè¿™æ˜¯ä¸€ä¸ªè€ç‚¹ (ä¸åœ¨ new_estimates é‡Œ)ï¼Œå®ƒå¯èƒ½å·²ç»è…åŒ–äº†
                # æˆ‘ä»¬ä¸ä»…è¦æ‹’ç»è¿™ä¸ªå› å­ï¼Œç”šè‡³åº”è¯¥è€ƒè™‘æŠŠè¿™ä¸ªç‚¹æ‹‰é»‘
            else:
                # é€šè¿‡æ£€æŸ¥ï¼ŒåŠ å…¥å¾…æ·»åŠ åˆ—è¡¨
                valid_visual_factors.append((kf_id, lm_id, pt_2d, depth, is_new_point))

        # -------------------------------------------------------------------------
        # æ­£å¼æ·»åŠ é€šè¿‡æ£€æŸ¥çš„å› å­åˆ° new_graph
        # -------------------------------------------------------------------------
        
        for kf_id, lm_id, pt_2d, depth, is_new_point in valid_visual_factors:
            # å¦‚æœè¿™ä¸ªç‚¹å·²ç»è¢«æ ‡è®°ä¸ºåç‚¹ï¼ˆå› ä¸ºåœ¨åˆ«çš„å¸§è§†è§’ä¸‹è¯¯å·®å·¨å¤§ï¼‰ï¼Œé‚£ä¹ˆå®ƒçš„æ‰€æœ‰å› å­éƒ½ä¸è¦äº†
            if lm_id in bad_landmarks:
                continue
                
            kf_gtsam_id = self._get_kf_gtsam_id(kf_id)
            lm_gtsam_id = self._get_lm_gtsam_id(lm_id)
            
            # ... (è¿™é‡Œæ”¾ä½ åŸæœ¬çš„æ„å»º factor ä»£ç ï¼Œä½¿ç”¨ Huber æ ¸ç­‰) ...
            # weighted_noise = self._get_adaptive_noise(depth, is_new_point)
            factor = gtsam.GenericProjectionFactorCal3_S2(
                    pt_2d, self.visual_robust_noise, X(kf_gtsam_id), L(lm_gtsam_id), 
                    self.K, body_P_sensor=self.body_T_cam
                )
            new_graph.add(factor)
            
            # æ›´æ–°æ—¶é—´æˆ³é€»è¾‘...
            if not is_new_point: # old_lm_exists
                 new_window_stamps[L(lm_gtsam_id)] = float(self._get_kf_gtsam_id(new_keyframe.get_id()))
            elif lm_id not in bad_landmarks:
                 valid_new_landmarks.add(lm_gtsam_id) # è¿™æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„æ–°ç‚¹

        # æ¸…ç†åƒåœ¾ï¼šæŠŠåˆšæ‰å‘ç°çš„ bad_landmarks ä» new_estimates é‡Œåˆ æ‰
        # é˜²æ­¢æŠŠæ²¡æœ‰å› å­çš„å­¤ç«‹ç‚¹åŠ è¿›å»ï¼Œå¯¼è‡´ Indeterminant
        for lm_id in bad_landmarks:
            lm_gtsam_id = self._get_lm_gtsam_id(lm_id)
            if new_estimates.exists(L(lm_gtsam_id)):
                print(f"ğŸ—‘ï¸ [Firewall] ç§»é™¤æœ‰æ¯’çš„æ–°ç‚¹å˜é‡ L{lm_id}")
                new_estimates.erase(L(lm_gtsam_id))
            if L(lm_gtsam_id) in new_window_stamps:
                del new_window_stamps[L(lm_gtsam_id)]
        
        # æ¸…ç†æ— æ•ˆçš„æ–°è·¯æ ‡ç‚¹
        # éå†æœ¬æ¬¡å°è¯•æ·»åŠ çš„æ‰€æœ‰æ–°è·¯æ ‡ç‚¹
        for lm_id in list(new_landmarks.keys()): 
            if lm_id not in self.landmark_id_to_gtsam_id: continue
            lm_gtsam_id = self._get_lm_gtsam_id(lm_id)

            # å¦‚æœå®ƒåœ¨ estimates é‡Œï¼ˆè¯´æ˜é€šè¿‡äº† NaN æ£€æŸ¥ï¼‰ï¼Œä½†ä¸åœ¨ valid é›†åˆé‡Œï¼ˆè¯´æ˜æ²¡å› å­ï¼‰
            if new_estimates.exists(L(lm_gtsam_id)) and lm_gtsam_id not in valid_new_landmarks:
                # print(f"ã€Backendã€‘: Cleaning up unconstrained new landmark L{lm_id} (All factors rejected)")
                new_estimates.erase(L(lm_gtsam_id))
                if L(lm_gtsam_id) in new_window_stamps:
                    del new_window_stamps[L(lm_gtsam_id)]

        #     print(f"ã€Backendã€‘: Added {len(new_landmarks)} new landmarks and {len(new_visual_factors)} visual factors.")
        # else:
        #     print("ã€Backendã€‘: Skipped visual landmarks and factors due to stationary state.")

        # ======================= ZERO-VELOCITY UPDATE (ZUPT) & NO-MOTION POSE FACTOR =======================
        if is_stationary:
            # æ·»åŠ é›¶é€Ÿåº¦æ›´æ–°å› å­
            last_kf_gtsam_id = self._get_kf_gtsam_id(last_keyframe.get_id())
            kf_gtsam_id = self._get_kf_gtsam_id(new_keyframe.get_id())
            zero_velocity_noise = gtsam.noiseModel.Isotropic.Sigma(3, 0.03)
            zero_velocity_prior = gtsam.PriorFactorVector(V(kf_gtsam_id), np.zeros(3), zero_velocity_noise)
            new_graph.add(zero_velocity_prior)
            print("ã€Backendã€‘: Added Zero-Velocity-Update (ZUPT) factor.")

            # # æ·»åŠ å•ä½ä½å§¿å› å­
            # no_motion_pose_noise = gtsam.noiseModel.Diagonal.Sigmas(
            #     np.array([0.01, 0.01, 0.01,  # æ—‹è½¬è½´ (roll, pitch, yaw)
            #               0.03, 0.03, 0.03])) # å¹³ç§» (x, y, z)
            
            # new_graph.add(gtsam.BetweenFactorPose3(X(last_kf_gtsam_id), X(kf_gtsam_id),      
            #               gtsam.Pose3(), no_motion_pose_noise))
            # print("ã€Backendã€‘: Added No-Motion Pose Factor.")
        # ============================================================================================

        # æ‰§è¡ŒiSAM2å¢é‡æ›´æ–°
        # graph = self.smoother.getFactors()
        # print("ã€Backendã€‘: graph: ", graph)
        # print(f"ã€Backendã€‘: Updating iSAM2 ({new_graph.size()} new factors, {new_estimates.size()} new variables)...")
        
        try:
            start_time = time.time()
            self.smoother.update(new_graph, new_estimates, new_window_stamps)
            end_time = time.time()
            print(f"ã€Backend Timerã€‘: Incremental optimization took { (end_time - start_time) * 1000:.3f} ms.")

        # except Exception as e:
        except RuntimeError as e:
            print("\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
            print("!!!!!!!!!! OPTIMIZATION FAILED !!!!!!!!!!!!!!")
            print(f"ERROR: {e}")
            return

        # æ›´æ–°æœ€æ–°bias
        latest_pose, latest_vel, latest_bias = self.get_latest_optimized_state()
        print(f"ã€Backendã€‘: Latest optimized state: pose: {latest_pose.matrix()}, velocity: {latest_vel}, bias: {latest_bias}")
        latest_gtsam_id = self.next_gtsam_kf_id - 1
        if latest_bias is not None:
            self.latest_bias = latest_bias

        if latest_pose is None:
             print("ã€Backendã€‘Critical: Optimization succeeded but state retrieval failed.")
             return

        # è®°å½•ä¼˜åŒ–è¯¯å·®
        new_factors_error = self._log_optimization_error(new_graph)
        self._log_state_and_errors(latest_gtsam_id, latest_pose, latest_vel, latest_bias, new_factors_error)

        print("ã€Backendã€‘: Incremental optimization complete.")


    def _log_optimization_error(self, new_factors_graph):
        try:
            optimized_result = self.smoother.calculateEstimate()
            new_factors_error = new_factors_graph.error(optimized_result)

            current_full_graph = self.smoother.getFactors()

            print(f"ã€Backendã€‘ä¼˜åŒ–è¯¯å·®ç»Ÿè®¡: "
                  f"æœ¬è½®æ–°å¢å› å­è¯¯å·® = {new_factors_error:.4f}")

            # ======================= DETAILED FACTOR ERROR LOGGING =======================
            debug_start_frame = 0 # è®¾ä¸º0ä»¥ç«‹å³å¼€å§‹æ‰“å°
            latest_gtsam_id = self.next_gtsam_kf_id - 1
            if latest_gtsam_id >= debug_start_frame:
                print("\n" + "="*40 + f" DETAILED ERROR ANALYSIS (Frame {latest_gtsam_id}) " + "="*40)
                
                # éå†å›¾ä¸­çš„æ‰€æœ‰å› å­
                for i in range(current_full_graph.size()):
                    factor = current_full_graph.at(i)
                    if factor is None: # æ£€æŸ¥å› å­æ˜¯å¦æœ‰æ•ˆ
                        continue
                        
                    try:
                        # è®¡ç®—è¿™ä¸ªç‰¹å®šå› å­çš„è¯¯å·®
                        error = factor.error(optimized_result)
                        
                        # æ‰“å°è¯¯å·®å¤§äºé˜ˆå€¼çš„å› å­ï¼Œä»¥é¿å…æ—¥å¿—åˆ·å±
                        if error > 10.0: 
                            # æ‰“å°å› å­çš„Pythonç±»å
                            factor_type = factor.__class__.__name__
                            print(f"  - Factor {i}: Error = {error:.4f}, Type = {factor_type}")
                            
                            # å°è¯•æ‰“å°ä¸è¯¥å› å­ç›¸å…³çš„Key
                            keys = factor.keys()
                            key_str = ", ".join([gtsam.DefaultKeyFormatter(key) for key in keys])
                            print(f"    Keys: [{key_str}]")
                            
                    except Exception as e_factor:
                        # æ•è·è®¡ç®—å•ä¸ªå› å­è¯¯å·®æ—¶å¯èƒ½å‘ç”Ÿçš„é”™è¯¯
                        print(f"  - Factor {i}: æ— æ³•è®¡ç®—è¯¯å·®æˆ–è·å–Keys. Error: {e_factor}")

                print("="*100 + "\n")
            # ===========================================================================
            
            return new_factors_error
            
        except Exception as e:
            print(f"[Error][Backend] è®¡ç®—ä¼˜åŒ–è¯¯å·®æ—¶å‡ºé”™: {e}")
            return -1.0, -1.0
        
    def _log_state_and_errors(self, latest_gtsam_id, latest_pose, latest_vel, latest_bias, new_factors_error):
        position = latest_pose.translation()
        acc_bias = latest_bias.accelerometer()
        gyro_bias = latest_bias.gyroscope()

        state_data = {
            "gtsam_id": latest_gtsam_id,
            "pos_x": position[0], "pos_y": position[1], "pos_z": position[2],
            "vel_x": latest_vel[0], "vel_y": latest_vel[1], "vel_z": latest_vel[2],
            "bias_acc_x": acc_bias[0], "bias_acc_y": acc_bias[1], "bias_acc_z": acc_bias[2],
            "bias_gyro_x": gyro_bias[0], "bias_gyro_y": gyro_bias[1], "bias_gyro_z": gyro_bias[2],
            "new_factors_error": new_factors_error
        }
        self.logger.log_state(state_data)

    def _get_adaptive_noise(self, depth, is_new_landmark):
        """
        ç»“åˆæ·±åº¦æƒé‡å’Œæ–°ç‚¹è†¨èƒ€çš„è‡ªé€‚åº”å™ªå£°æ¨¡å‹
        depth: landmarkåˆ°ç›¸æœºçš„æ·±åº¦ï¼ˆç±³ï¼‰
        is_new_landmark: æ˜¯å¦ä¸ºåˆšå…¥å›¾çš„æ–°ç‚¹
        """
        # 1. ç¬¬ä¸€å±‚ï¼šè®¡ç®—åŸºäºæ·±åº¦çš„åŸºç¡€å™ªå£° (Base Sigma)
        if depth is None:
            base_sigma = 2.0
        else:
            if depth <= self.depth_weight_base:
                base_sigma = 2.0 # åŸºç¡€åƒç´ å™ªå£°
            else:
                # æ·±åº¦è¶Šè¿œï¼Œå™ªå£°è¶Šå¤§
                depth_ratio = depth / self.depth_weight_base
                # é™åˆ¶ä¸€ä¸‹æœ€å¤§æ·±åº¦å€æ•°ï¼Œé˜²æ­¢æ— ç©·è¿œç‚¹å¯¼è‡´æ•°å€¼é—®é¢˜
                clamped_ratio = min(depth_ratio, 5.0) 
                weight_factor = 1.0 + (clamped_ratio ** self.depth_weight_power) * (self.depth_weight_max - 1.0)
                base_sigma = 2.0 * weight_factor
        
        # 2. ç¬¬äºŒå±‚ï¼šå¦‚æœæ˜¯æ–°ç‚¹ï¼Œåº”ç”¨è†¨èƒ€ç³»æ•° (Inflation)
        if is_new_landmark:
            final_sigma = base_sigma * self.new_landmark_inflation_ratio
        else:
            final_sigma = base_sigma
            
        # 3. åˆ›å»º Huber é²æ£’æ ¸å™ªå£°æ¨¡å‹
        noise_model = gtsam.noiseModel.Isotropic.Sigma(2, final_sigma)
        robust_noise = gtsam.noiseModel.Robust.Create(
            gtsam.noiseModel.mEstimator.Huber.Create(2.5), 
            noise_model
        )
        return robust_noise

    def _compute_landmark_depth(self, lm_3d_pos, kf_pose):
        # è·å–bodyåˆ°ç›¸æœºçš„å˜æ¢
        T_bc = self.T_bc
        R_bc = T_bc[:3, :3]
        t_bc = T_bc[:3, 3]
        
        # è®¡ç®—ç›¸æœºåœ¨ä¸–ç•Œåæ ‡ç³»ä¸‹çš„ä½ç½®
        T_w_b = kf_pose.matrix()
        R_w_b = T_w_b[:3, :3]
        t_w_b = T_w_b[:3, 3]
        
        # ç›¸æœºä½ç½® = bodyä½ç½® + R_w_b @ t_bc
        cam_pos_w = t_w_b + R_w_b @ t_bc
        
        # è®¡ç®—æ·±åº¦ï¼ˆä¸–ç•Œåæ ‡ç³»ä¸‹çš„è·ç¦»ï¼‰
        # ä¿®å¤ï¼šä½¿ç”¨ try-except è€Œä¸æ˜¯ isinstanceï¼Œå› ä¸º gtsam.Point3 å¯èƒ½ä¸å¯ç›´æ¥è®¿é—®
        try:
            # å°è¯•ä½¿ç”¨ x(), y(), z() æ–¹æ³•ï¼ˆGTSAM Point3å¯¹è±¡ï¼‰
            lm_pos_w = np.array([lm_3d_pos.x(), lm_3d_pos.y(), lm_3d_pos.z()])
        except AttributeError:
            # å¦‚æœä¸æ˜¯Point3å¯¹è±¡ï¼Œç›´æ¥è½¬æ¢ä¸ºnumpyæ•°ç»„
            lm_pos_w = np.array(lm_3d_pos)
        
        depth = np.linalg.norm(lm_pos_w - cam_pos_w)
        return depth
